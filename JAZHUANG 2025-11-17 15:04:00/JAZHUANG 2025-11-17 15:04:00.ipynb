{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "lawdkd7eurztpn3w5vj7",
   "authorId": "501304564857",
   "authorName": "JAZHUANG",
   "authorEmail": "jay.zhuang@snowflake.com",
   "sessionId": "9cdc583f-d190-46aa-bc9c-96614f3de7d4",
   "lastEditTime": 1764632037527
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n\ndf = pd.read_csv('JAZHUANG 2025-11-17 15:04:00/bench1.csv')\nst.dataframe(df)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e004df94-1d53-4789-bbc0-c94cc8d1e516",
   "metadata": {
    "language": "python",
    "name": "latency_investigation"
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Load the data\ndf = pd.read_csv('JAZHUANG 2025-11-17 15:04:00/bench4.csv')\n\n# Sort by timestamp to ensure proper ordering\ndf = df.sort_values('Timestamp').reset_index(drop=True)\n\n# Create elapsed time from first timestamp (normalized to start at 0)\n# Using raw timestamp values as they are already in seconds\nfirst_timestamp = df['Timestamp'].min()\ndf['ElapsedSeconds'] = df['Timestamp'] - first_timestamp\n\n# Method 1: Using Streamlit's native charting with elapsed time\nst.subheader(\"Latency Over Time by Type (Elapsed Time)\")\n\n# Pivot the data for streamlit line_chart using elapsed seconds\npivot_df = df.pivot_table(index='ElapsedSeconds', columns='Type', values='LatencyMs', aggfunc='mean').reset_index()\nst.line_chart(pivot_df.set_index('ElapsedSeconds'))\n\n# Method 2: Using matplotlib (more control) with elapsed time and separate medians\nst.subheader(\"Latency Over Time (Matplotlib - Elapsed Time with Type-Specific Medians)\")\n\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Define colors for consistency\ncolors = {'Range': 'blue', 'Value': 'orange'}\nmedian_colors = {'Range': 'darkblue', 'Value': 'darkorange'}\n\n# Plot separate lines for each Type using elapsed seconds\nfor type_name in df['Type'].unique():\n    type_data = df[df['Type'] == type_name]\n    ax.plot(type_data['ElapsedSeconds'], type_data['LatencyMs'], \n            label=f'{type_name}', color=colors.get(type_name, 'gray'), \n            marker='o', markersize=0.3, linewidth=0.6, alpha=0.6)\n\n# Calculate and plot median latency per second for each type separately\ndf['ElapsedSecondsRounded'] = np.floor(df['ElapsedSeconds']).astype(int)\n\nmedian_per_second_by_type = {}\nfor type_name in df['Type'].unique():\n    type_data = df[df['Type'] == type_name]\n    median_data = type_data.groupby('ElapsedSecondsRounded')['LatencyMs'].median().reset_index()\n    median_per_second_by_type[type_name] = median_data\n    \n    # Plot median line for this type\n    ax.plot(median_data['ElapsedSecondsRounded'], median_data['LatencyMs'], \n            label=f'{type_name} Median', color=median_colors.get(type_name, 'black'), \n            linewidth=2.5, marker='s', markersize=4, alpha=0.9)\n\nax.set_xlabel('Elapsed Time (seconds)')\nax.set_ylabel('Latency (Ms)')\nax.set_title('Latency Over Time by Type with Separate Medians per Second')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nst.pyplot(fig)\n\n# Display timing information\nst.subheader(\"Timing Information\")\nst.write(f\"First timestamp: {first_timestamp}\")\nst.write(f\"Last timestamp: {df['Timestamp'].max()}\")\nst.write(f\"Total duration: {df['ElapsedSeconds'].max():.6f} seconds\")\nst.write(f\"Total duration: {df['ElapsedSeconds'].max()/60:.6f} minutes\")\n\n# Display data summary\nst.subheader(\"Data Summary\")\nst.write(f\"Total records: {len(df)}\")\nst.write(\"Records by Type:\")\nst.write(df['Type'].value_counts())\n\n# Show median statistics per second by type\nst.subheader(\"Median Latency Statistics by Type\")\nfor type_name in df['Type'].unique():\n    type_data = df[df['Type'] == type_name]\n    median_data = median_per_second_by_type[type_name]\n    \n    st.write(f\"**{type_name} Type:**\")\n    st.write(f\"  - Overall median latency: {type_data['LatencyMs'].median():.3f} ms\")\n    st.write(f\"  - Median per second - Min: {median_data['LatencyMs'].min():.3f} ms\")\n    st.write(f\"  - Median per second - Max: {median_data['LatencyMs'].max():.3f} ms\")\n    st.write(f\"  - Median per second - Average: {median_data['LatencyMs'].mean():.3f} ms\")\n    st.write(\"\")\n\n# Show raw data with elapsed time\nst.subheader(\"Raw Data Preview (with Elapsed Time)\")\ndisplay_df = df[['Timestamp', 'ElapsedSeconds', 'LatencyMs', 'Type']].head(20)\nst.dataframe(display_df)\n\n# Show median per second data for each type\nst.subheader(\"Median Latency per Second by Type (Preview)\")\nfor type_name in df['Type'].unique():\n    st.write(f\"**{type_name} Median per Second:**\")\n    st.dataframe(median_per_second_by_type[type_name].head(10))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab8a07f2-d568-4525-bd77-53f123cd8489",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Load the data\ndf = pd.read_csv('JAZHUANG 2025-11-17 15:04:00/bencha0.csv')\n\n# Check available columns\nst.write(\"**Available columns:**\", list(df.columns))\n\n# Remove outliers: filter out p99.9 values for latency\np99_9 = df['LatencyMs'].quantile(0.999)\noriginal_count = len(df)\ndf = df[df['LatencyMs'] <= p99_9].copy()\nfiltered_count = len(df)\n\nst.write(f\"**Outlier Filtering:**\")\nst.write(f\"- P99.9 threshold: {p99_9:.3f} ms\")\nst.write(f\"- Original records: {original_count:,}\")\nst.write(f\"- After filtering: {filtered_count:,}\")\nst.write(f\"- Removed {original_count - filtered_count:,} outliers ({((original_count - filtered_count) / original_count * 100):.2f}%)\")\nst.write(\"\")\n\n# Sort by timestamp to ensure proper ordering\ndf = df.sort_values('Timestamp').reset_index(drop=True)\n\n# Create elapsed time from first timestamp (normalized to start at 0)\nfirst_timestamp = df['Timestamp'].min()\ndf['ElapsedSeconds'] = df['Timestamp'] - first_timestamp\n\n# Calculate delta if PTree columns exist\nif 'PTreeAllocated' in df.columns and 'PTreeReleased' in df.columns:\n    df['PTreeDelta'] = df['PTreeAllocated'] - df['PTreeReleased']\n    st.write(\"✅ PTree columns found - calculating delta (PTreeAllocated - PTreeReleased)\")\nelse:\n    st.write(\"⚠️ PTree columns not found in the data\")\n\n# Method 1: Latency Analysis with Streamlit\nst.subheader(\"Latency Over Time by Type (Elapsed Time) - Outliers Removed\")\n\n# Pivot the data for streamlit line_chart using elapsed seconds\npivot_df = df.pivot_table(index='ElapsedSeconds', columns='Type', values='LatencyMs', aggfunc='mean').reset_index()\nst.line_chart(pivot_df.set_index('ElapsedSeconds'))\n\n# Method 2: Combined Latency and PTreeDelta Analysis\nif 'PTreeAllocated' in df.columns and 'PTreeReleased' in df.columns:\n    st.subheader(\"Latency and PTree Delta Analysis (Combined)\")\n    \n    fig, ax1 = plt.subplots(figsize=(16, 10))\n    \n    # Define colors for consistency\n    colors = {'Range': 'blue', 'Value': 'orange'}\n    median_colors = {'Range': 'darkblue', 'Value': 'darkorange'}\n    \n    # Plot latency lines for each Type on primary y-axis\n    for type_name in df['Type'].unique():\n        type_data = df[df['Type'] == type_name]\n        ax1.plot(type_data['ElapsedSeconds'], type_data['LatencyMs'], \n                label=f'{type_name} Latency', color=colors.get(type_name, 'gray'), \n                marker='o', markersize=0.3, linewidth=0.6, alpha=0.6)\n    \n    # Calculate and plot median latency per second for each type\n    df['ElapsedSecondsRounded'] = np.floor(df['ElapsedSeconds']).astype(int)\n    \n    for type_name in df['Type'].unique():\n        type_data = df[df['Type'] == type_name]\n        median_data = type_data.groupby('ElapsedSecondsRounded')['LatencyMs'].median().reset_index()\n        \n        # Plot median line for this type\n        ax1.plot(median_data['ElapsedSecondsRounded'], median_data['LatencyMs'], \n                label=f'{type_name} Latency Median', color=median_colors.get(type_name, 'black'), \n                linewidth=2.5, marker='s', markersize=4, alpha=0.9)\n    \n    # Set up primary y-axis (latency)\n    ax1.set_xlabel('Elapsed Time (seconds)')\n    ax1.set_ylabel('Latency (Ms)', color='black')\n    ax1.tick_params(axis='y', labelcolor='black')\n    ax1.grid(True, alpha=0.3)\n    \n    # Create secondary y-axis for PTreeDelta\n    ax2 = ax1.twinx()\n    \n    # Plot PTreeDelta (without separating by type)\n    ax2.plot(df['ElapsedSeconds'], df['PTreeDelta'], \n            label='PTree Delta (Allocated - Released)', color='red', \n            linewidth=1.5, alpha=0.8)\n    \n    # Calculate and plot median PTreeDelta per second\n    ptree_median_data = df.groupby('ElapsedSecondsRounded')['PTreeDelta'].median().reset_index()\n    ax2.plot(ptree_median_data['ElapsedSecondsRounded'], ptree_median_data['PTreeDelta'], \n            label='PTree Delta Median', color='darkred', \n            linewidth=3, marker='D', markersize=5, alpha=0.9)\n    \n    # Set up secondary y-axis (PTreeDelta)\n    ax2.set_ylabel('PTree Delta (bytes)', color='red')\n    ax2.tick_params(axis='y', labelcolor='red')\n    \n    # Combine legends from both axes\n    lines1, labels1 = ax1.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', bbox_to_anchor=(0.02, 0.98))\n    \n    ax1.set_title('Latency and PTree Delta Over Time (P99.9 Outliers Removed)')\n    \n    plt.tight_layout()\n    st.pyplot(fig)\n    \n    # PTreeDelta Statistics\n    st.subheader(\"PTree Delta Statistics\")\n    st.write(f\"**Overall PTree Delta:**\")\n    st.write(f\"  - Mean: {df['PTreeDelta'].mean():.0f} bytes\")\n    st.write(f\"  - Median: {df['PTreeDelta'].median():.0f} bytes\")\n    st.write(f\"  - Max: {df['PTreeDelta'].max():.0f} bytes\")\n    st.write(f\"  - Min: {df['PTreeDelta'].min():.0f} bytes\")\n    st.write(f\"  - Std deviation: {df['PTreeDelta'].std():.0f} bytes\")\n    st.write(\"\")\n\nelse:\n    # Fallback to original latency-only plot if PTree columns don't exist\n    st.subheader(\"Latency Analysis (Matplotlib)\")\n    \n    fig, ax = plt.subplots(figsize=(14, 8))\n    \n    # Define colors for consistency\n    colors = {'Range': 'blue', 'Value': 'orange'}\n    median_colors = {'Range': 'darkblue', 'Value': 'darkorange'}\n    \n    # Plot separate lines for each Type using elapsed seconds\n    for type_name in df['Type'].unique():\n        type_data = df[df['Type'] == type_name]\n        ax.plot(type_data['ElapsedSeconds'], type_data['LatencyMs'], \n                label=f'{type_name}', color=colors.get(type_name, 'gray'), \n                marker='o', markersize=0.3, linewidth=0.6, alpha=0.6)\n    \n    # Calculate and plot median latency per second for each type separately\n    df['ElapsedSecondsRounded'] = np.floor(df['ElapsedSeconds']).astype(int)\n    \n    median_per_second_by_type = {}\n    for type_name in df['Type'].unique():\n        type_data = df[df['Type'] == type_name]\n        median_data = type_data.groupby('ElapsedSecondsRounded')['LatencyMs'].median().reset_index()\n        median_per_second_by_type[type_name] = median_data\n        \n        # Plot median line for this type\n        ax.plot(median_data['ElapsedSecondsRounded'], median_data['LatencyMs'], \n                label=f'{type_name} Median', color=median_colors.get(type_name, 'black'), \n                linewidth=2.5, marker='s', markersize=4, alpha=0.9)\n    \n    ax.set_xlabel('Elapsed Time (seconds)')\n    ax.set_ylabel('Latency (Ms)')\n    ax.set_title('Latency Over Time by Type with Separate Medians per Second (P99.9 Outliers Removed)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    st.pyplot(fig)\n\n# Display timing information\nst.subheader(\"Timing Information\")\nst.write(f\"First timestamp: {first_timestamp}\")\nst.write(f\"Last timestamp: {df['Timestamp'].max()}\")\nst.write(f\"Total duration: {df['ElapsedSeconds'].max():.6f} seconds\")\nst.write(f\"Total duration: {df['ElapsedSeconds'].max()/60:.6f} minutes\")\n\n# Display data summary\nst.subheader(\"Data Summary (After Outlier Removal)\")\nst.write(f\"Total records: {len(df):,}\")\nst.write(\"Records by Type:\")\nst.write(df['Type'].value_counts())\n\n# Show percentile information\nst.subheader(\"Latency Percentile Information\")\npercentiles = [50, 90, 95, 99, 99.9]\nfor p in percentiles:\n    st.write(f\"P{p}: {df['LatencyMs'].quantile(p/100):.3f} ms\")\n\n# Show median statistics per second by type\nst.subheader(\"Median Latency Statistics by Type (Outliers Removed)\")\nif 'median_per_second_by_type' in locals():\n    for type_name in df['Type'].unique():\n        type_data = df[df['Type'] == type_name]\n        median_data = median_per_second_by_type[type_name]\n        \n        st.write(f\"**{type_name} Type:**\")\n        st.write(f\"  - Overall median latency: {type_data['LatencyMs'].median():.3f} ms\")\n        st.write(f\"  - Mean latency: {type_data['LatencyMs'].mean():.3f} ms\")\n        st.write(f\"  - Std deviation: {type_data['LatencyMs'].std():.3f} ms\")\n        st.write(f\"  - Median per second - Min: {median_data['LatencyMs'].min():.3f} ms\")\n        st.write(f\"  - Median per second - Max: {median_data['LatencyMs'].max():.3f} ms\")\n        st.write(f\"  - Median per second - Average: {median_data['LatencyMs'].mean():.3f} ms\")\n        st.write(\"\")\n\n# Show raw data with elapsed time\nst.subheader(\"Raw Data Preview (with Elapsed Time, Outliers Removed)\")\nif 'PTreeAllocated' in df.columns and 'PTreeReleased' in df.columns:\n    display_df = df[['Timestamp', 'ElapsedSeconds', 'LatencyMs', 'PTreeAllocated', 'PTreeReleased', 'PTreeDelta', 'Type']].head(20)\nelse:\n    display_df = df[['Timestamp', 'ElapsedSeconds', 'LatencyMs', 'Type']].head(20)\nst.dataframe(display_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5cc9f3c-9db3-4594-a0de-c95482e919c6",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Load the JSON data\ntry:\n    # Try reading as JSON first\n    with open('JAZHUANG 2025-11-17 15:04:00/bench60_2.csv', 'r') as f:\n        data = []\n        for line in f:\n            if line.strip():  # Skip empty lines\n                data.append(json.loads(line))\n        df = pd.DataFrame(data)\n    st.write(\"✅ Successfully loaded JSON data\")\nexcept json.JSONDecodeError:\n    # If JSON fails, try as CSV\n    try:\n        df = pd.read_csv('JAZHUANG 2025-11-17 15:04:00/benchb0_2.csv')\n        st.write(\"✅ Successfully loaded CSV data\")\n    except Exception as e:\n        st.error(f\"Failed to load data: {e}\")\n        st.stop()\n\n# Check available columns\nst.write(\"**Available columns:**\", list(df.columns))\n\n# Check if required columns exist\nrequired_columns = ['ReadVersion', 'TotalUs']\noptional_columns = ['StartWaitUs']\nmissing_required = [col for col in required_columns if col not in df.columns]\n\nif missing_required:\n    st.error(f\"Required columns {missing_required} not found in the data\")\n    st.write(\"Available columns:\", list(df.columns))\n    st.stop()\n\n# Check for StartWaitUs column\nhas_start_wait = 'StartWaitUs' in df.columns\nif has_start_wait:\n    st.write(\"✅ StartWaitUs column found - will include in analysis\")\nelse:\n    st.write(\"⚠️ StartWaitUs column not found - proceeding with TotalUs only\")\n\n# Remove outliers: filter out p99.9 values for TotalUs\np99_9_total = df['TotalUs'].quantile(0.999)\noriginal_count = len(df)\ndf = df[df['TotalUs'] <= p99_9_total].copy()\n\n# Also filter StartWaitUs outliers if the column exists\nif has_start_wait:\n    p99_9_start = df['StartWaitUs'].quantile(0.999)\n    df = df[df['StartWaitUs'] <= p99_9_start].copy()\n    \nfiltered_count = len(df)\n\nst.write(f\"**Outlier Filtering:**\")\nst.write(f\"- TotalUs P99.9 threshold: {p99_9_total:.3f} µs\")\nif has_start_wait:\n    st.write(f\"- StartWaitUs P99.9 threshold: {p99_9_start:.3f} µs\")\nst.write(f\"- Original records: {original_count:,}\")\nst.write(f\"- After filtering: {filtered_count:,}\")\nst.write(f\"- Removed {original_count - filtered_count:,} outliers ({((original_count - filtered_count) / original_count * 100):.2f}%)\")\nst.write(\"\")\n\n# Sort by ReadVersion to ensure proper ordering\ndf = df.sort_values('ReadVersion').reset_index(drop=True)\n\n# Create normalized ReadVersion starting from 0\nfirst_version = df['ReadVersion'].min()\ndf['NormalizedReadVersion'] = df['ReadVersion'] - first_version\n\nst.write(f\"**Data Summary (After Outlier Removal):**\")\nst.write(f\"- Total records: {len(df):,}\")\nst.write(f\"- ReadVersion range: {df['ReadVersion'].min():,} to {df['ReadVersion'].max():,}\")\nst.write(f\"- TotalUs range: {df['TotalUs'].min():.2f} to {df['TotalUs'].max():.2f}\")\nif has_start_wait:\n    st.write(f\"- StartWaitUs range: {df['StartWaitUs'].min():.2f} to {df['StartWaitUs'].max():.2f}\")\nst.write(\"\")\n\n# Method 1: Streamlit native charting\nif has_start_wait:\n    st.subheader(\"TotalUs and StartWaitUs Over ReadVersion (Streamlit Charts) - Outliers Removed\")\n    \n    # Create charts for both metrics\n    chart_df = df.set_index('NormalizedReadVersion')[['TotalUs', 'StartWaitUs']]\n    st.line_chart(chart_df)\nelse:\n    st.subheader(\"TotalUs Over ReadVersion (Streamlit Chart) - Outliers Removed\")\n    \n    # Create a simple line chart\n    chart_df = df.set_index('NormalizedReadVersion')[['TotalUs']]\n    st.line_chart(chart_df)\n\n# Method 2: Combined Analysis with Matplotlib\ninterval_size = 1000000\ndf['ReadVersionInterval'] = (df['NormalizedReadVersion'] // interval_size) * interval_size\n\nif has_start_wait:\n    st.subheader(\"TotalUs and StartWaitUs Analysis with Medians - Outliers Removed\")\n    \n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))\n    \n    # Plot 1: TotalUs\n    ax1.scatter(df['NormalizedReadVersion'], df['TotalUs'], \n               color='blue', alpha=0.6, s=1, label='TotalUs')\n    \n    # Calculate median TotalUs per interval\n    median_total_data = df.groupby('ReadVersionInterval')['TotalUs'].median().reset_index()\n    median_total_data['IntervalMidpoint'] = median_total_data['ReadVersionInterval'] + (interval_size / 2)\n    \n    # Plot median line for TotalUs\n    ax1.plot(median_total_data['IntervalMidpoint'], median_total_data['TotalUs'], \n             color='darkblue', linewidth=3, marker='o', markersize=6, \n             label=f'Median per {interval_size:,} ReadVersion')\n    \n    ax1.set_xlabel('Normalized ReadVersion')\n    ax1.set_ylabel('TotalUs (µs)', color='blue')\n    ax1.tick_params(axis='y', labelcolor='blue')\n    ax1.grid(True, alpha=0.3)\n    ax1.legend(loc='upper left')\n    ax1.set_title('TotalUs Over ReadVersion with Median per 1M Interval')\n    ax1.ticklabel_format(style='plain', axis='x')\n    \n    # Plot 2: StartWaitUs\n    ax2.scatter(df['NormalizedReadVersion'], df['StartWaitUs'], \n               color='orange', alpha=0.6, s=1, label='StartWaitUs')\n    \n    # Calculate median StartWaitUs per interval\n    median_start_data = df.groupby('ReadVersionInterval')['StartWaitUs'].median().reset_index()\n    median_start_data['IntervalMidpoint'] = median_start_data['ReadVersionInterval'] + (interval_size / 2)\n    \n    # Plot median line for StartWaitUs\n    ax2.plot(median_start_data['IntervalMidpoint'], median_start_data['StartWaitUs'], \n             color='darkorange', linewidth=3, marker='s', markersize=6, \n             label=f'Median per {interval_size:,} ReadVersion')\n    \n    ax2.set_xlabel('Normalized ReadVersion')\n    ax2.set_ylabel('StartWaitUs (µs)', color='orange')\n    ax2.tick_params(axis='y', labelcolor='orange')\n    ax2.grid(True, alpha=0.3)\n    ax2.legend(loc='upper left')\n    ax2.set_title('StartWaitUs Over ReadVersion with Median per 1M Interval')\n    ax2.ticklabel_format(style='plain', axis='x')\n    plt.setp(ax2.get_xticklabels(), rotation=45)\n    \n    plt.tight_layout()\n    st.pyplot(fig)\n    \nelse:\n    st.subheader(\"TotalUs Analysis with Median - Outliers Removed\")\n    \n    fig, ax1 = plt.subplots(figsize=(16, 10))\n    \n    # Plot raw TotalUs data on primary y-axis\n    ax1.scatter(df['NormalizedReadVersion'], df['TotalUs'], \n               color='blue', alpha=0.6, s=1, label='TotalUs')\n    \n    # Calculate median TotalUs per interval\n    median_total_data = df.groupby('ReadVersionInterval')['TotalUs'].median().reset_index()\n    median_total_data['IntervalMidpoint'] = median_total_data['ReadVersionInterval'] + (interval_size / 2)\n    \n    # Plot median line\n    ax1.plot(median_total_data['IntervalMidpoint'], median_total_data['TotalUs'], \n             color='darkblue', linewidth=3, marker='o', markersize=6, \n             label=f'Median per {interval_size:,} ReadVersion')\n    \n    # Set up primary y-axis\n    ax1.set_xlabel('Normalized ReadVersion')\n    ax1.set_ylabel('TotalUs (µs)', color='blue')\n    ax1.tick_params(axis='y', labelcolor='blue')\n    ax1.grid(True, alpha=0.3)\n    ax1.legend(loc='upper left')\n    \n    # Format x-axis to show numbers in a readable format\n    ax1.ticklabel_format(style='plain', axis='x')\n    plt.setp(ax1.get_xticklabels(), rotation=45)\n    \n    ax1.set_title('TotalUs Over ReadVersion with Median per 1M Interval (P99.9 Outliers Removed)')\n    \n    plt.tight_layout()\n    st.pyplot(fig)\n\n# Show percentile information\nst.subheader(\"Percentile Information\")\npercentiles = [50, 90, 95, 99, 99.9]\n\nst.write(\"**TotalUs Percentiles:**\")\nfor p in percentiles:\n    st.write(f\"P{p}: {df['TotalUs'].quantile(p/100):.3f} µs\")\n\nif has_start_wait:\n    st.write(\"**StartWaitUs Percentiles:**\")\n    for p in percentiles:\n        st.write(f\"P{p}: {df['StartWaitUs'].quantile(p/100):.3f} µs\")\n\n# Statistics\nst.subheader(\"Statistics (After Outlier Removal)\")\n\nst.write(f\"**TotalUs Statistics:**\")\nst.write(f\"  - Mean: {df['TotalUs'].mean():.3f} µs\")\nst.write(f\"  - Median: {df['TotalUs'].median():.3f} µs\")\nst.write(f\"  - Max: {df['TotalUs'].max():.3f} µs\")\nst.write(f\"  - Min: {df['TotalUs'].min():.3f} µs\")\nst.write(f\"  - Std deviation: {df['TotalUs'].std():.3f} µs\")\nst.write(\"\")\n\nif has_start_wait:\n    st.write(f\"**StartWaitUs Statistics:**\")\n    st.write(f\"  - Mean: {df['StartWaitUs'].mean():.3f} µs\")\n    st.write(f\"  - Median: {df['StartWaitUs'].median():.3f} µs\")\n    st.write(f\"  - Max: {df['StartWaitUs'].max():.3f} µs\")\n    st.write(f\"  - Min: {df['StartWaitUs'].min():.3f} µs\")\n    st.write(f\"  - Std deviation: {df['StartWaitUs'].std():.3f} µs\")\n    st.write(\"\")\n\n# Median interval statistics\nst.write(f\"**Median per {interval_size:,} ReadVersion intervals:**\")\nst.write(f\"  - Number of intervals: {len(median_total_data)}\")\nst.write(f\"  - TotalUs median range: {median_total_data['TotalUs'].min():.3f} to {median_total_data['TotalUs'].max():.3f} µs\")\nst.write(f\"  - TotalUs average of medians: {median_total_data['TotalUs'].mean():.3f} µs\")\n\nif has_start_wait:\n    st.write(f\"  - StartWaitUs median range: {median_start_data['StartWaitUs'].min():.3f} to {median_start_data['StartWaitUs'].max():.3f} µs\")\n    st.write(f\"  - StartWaitUs average of medians: {median_start_data['StartWaitUs'].mean():.3f} µs\")\n\n# Show median data per interval\nst.subheader(\"Median Values per Interval (Outliers Removed)\")\n\nif has_start_wait:\n    # Combine both median datasets\n    combined_median = median_total_data.merge(median_start_data, on=['ReadVersionInterval', 'IntervalMidpoint'], suffixes=('_Total', '_Start'))\n    combined_display = combined_median.copy()\n    combined_display['ReadVersionInterval'] = combined_display['ReadVersionInterval'].apply(lambda x: f\"{x:,}\")\n    combined_display['TotalUs'] = combined_display['TotalUs'].round(3)\n    combined_display['StartWaitUs_Start'] = combined_display['StartWaitUs_Start'].round(3)\n    st.dataframe(combined_display[['ReadVersionInterval', 'TotalUs', 'StartWaitUs_Start']].rename(columns={\n        'TotalUs': 'TotalUs_Median',\n        'StartWaitUs_Start': 'StartWaitUs_Median'\n    }))\nelse:\n    median_display = median_total_data.copy()\n    median_display['ReadVersionInterval'] = median_display['ReadVersionInterval'].apply(lambda x: f\"{x:,}\")\n    median_display['TotalUs'] = median_display['TotalUs'].round(3)\n    st.dataframe(median_display[['ReadVersionInterval', 'TotalUs']])\n\n# Show raw data preview\nst.subheader(\"Raw Data Preview (Outliers Removed)\")\nif has_start_wait:\n    display_df = df[['ReadVersion', 'NormalizedReadVersion', 'TotalUs', 'StartWaitUs', 'ReadVersionInterval']].head(20)\nelse:\n    display_df = df[['ReadVersion', 'NormalizedReadVersion', 'TotalUs', 'ReadVersionInterval']].head(20)\nst.dataframe(display_df)\n\n",
   "execution_count": null
  }
 ]
}