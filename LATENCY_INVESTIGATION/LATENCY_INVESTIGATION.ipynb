{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "hhzrzwwdkd6mg5ednanv",
   "authorId": "501304564857",
   "authorName": "JAZHUANG",
   "authorEmail": "jay.zhuang@snowflake.com",
   "sessionId": "3fe81d48-829d-4eb9-a0a2-d9fc59a6f341",
   "lastEditTime": 1763681888310
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec86d7d-ac05-4de1-9ac3-fcd84cc0704c",
   "metadata": {
    "name": "overview",
    "collapsed": false
   },
   "source": "# Unistore Latency Spikes Investigation"
  },
  {
   "cell_type": "code",
   "id": "d493ee98-aa09-4ed6-b692-53a5048525b8",
   "metadata": {
    "language": "sql",
    "name": "latency"
   },
   "outputs": [],
   "source": "SELECT\n    TIME,\n    MACHINE,\n    v:\"P99\"::float as latency_p99\nFROM\n    SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\nWHERE\n    Type = 'NormalReadLatencyMetrics'\n    AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n    AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n    AND '2025-11-06 00:00:00'::timestamp_ntz;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "latency_spikes"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\ndf = latency.to_pandas()\n\n# Display basic info about the dataset\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Columns: {list(df.columns)}\")\n\n# Show first few rows\nprint(\"\\nFirst 5 rows:\")\nprint(df.head())\n\n# Plot latency over time as scatter plot (points without connecting lines)\nplt.figure(figsize=(12, 6))\n\n# Create boolean mask for high latency\nhigh_latency_mask = df['LATENCY_P99'] > 0.5\n\n# Plot normal latency in blue\nnormal_data = df[~high_latency_mask]\nif not normal_data.empty:\n    plt.scatter(normal_data['TIME'], normal_data['LATENCY_P99'], \n               color='blue', alpha=0.7, label='Normal (<= 0.5s)', s=20)\n\n# Plot high latency in red\nhigh_data = df[high_latency_mask]\nif not high_data.empty:\n    plt.scatter(high_data['TIME'], high_data['LATENCY_P99'], \n               color='red', alpha=0.7, label='High (> 0.5s)', s=20)\n\nplt.xlabel('Time')\nplt.ylabel('Latency P99 (seconds)')\nplt.title('prod1_autoprovision_88_100 P99 Latency Over Time (1 week)')\nplt.xticks(rotation=45)\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Also create a Streamlit version\nst.subheader(\"prod1_autoprovision_88_100 P99 Latency 1 week\")\nst.line_chart(df.set_index('TIME')['LATENCY_P99'])\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b9496a52-54a8-4815-8a2c-2f0cff162631",
   "metadata": {
    "language": "sql",
    "name": "latency_get_range"
   },
   "outputs": [],
   "source": "WITH high_latency_events AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:\"P99\"::float as latency_p99\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'NormalReadLatencyMetrics'\n        AND v:\"P99\"::float > 0.5\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n),\nredwood_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:OpGetRange::float as op_get_range,\n        COALESCE(v:OpTryNext::float, 0) + \n        COALESCE(v:OpNext::float, 0) + \n        COALESCE(v:OpNextBatch::float, 0) as op_nexts\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'RedwoodMetrics'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n),\nstorage_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        split_part(v:ClearRangeMutations, ' ', 1)::float as clear_range_mutations,\n        split_part(v:GetRangeQueries, ' ', 1)::float as ss_get_range\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'StorageMetrics'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n        AND v:ClearRangeMutations IS NOT NULL\n),\nread_version_wait_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:\"P99\"::float as read_version_wait_p99\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'ReadVersionWaitMetrics'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n        AND v:\"P99\" IS NOT NULL\n),\nread_queue_wait_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:\"P99\"::float as read_queue_wait_p99\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'ReadQueueWaitMetrics'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n        AND v:\"P99\" IS NOT NULL\n)\nSELECT\n    h.TIME as high_latency_time,\n    h.MACHINE,\n    h.latency_p99,\n    SUM(r.op_get_range) as total_op_get_range_1min_before,\n    SUM(r.op_nexts) as total_op_nexts_1min_before,\n    SUM(s.clear_range_mutations) as total_clear_range_mutations_1min_before,\n    SUM(s.ss_get_range) as total_ss_get_range_1min_before,\n    MAX(rv.read_version_wait_p99) as max_read_version_wait_p99_1min_before,\n    MAX(rq.read_queue_wait_p99) as max_read_queue_wait_p99_1min_before\nFROM\n    high_latency_events h\n    LEFT JOIN redwood_metrics r ON h.MACHINE = r.MACHINE\n        AND r.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('second', 5, h.TIME)\n    LEFT JOIN storage_metrics s ON h.MACHINE = s.MACHINE\n        AND s.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('second', 5, h.TIME)\n    LEFT JOIN read_version_wait_metrics rv ON h.MACHINE = rv.MACHINE\n        AND rv.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('minute', 1, h.TIME)\n    LEFT JOIN read_queue_wait_metrics rq ON h.MACHINE = rq.MACHINE\n        AND rq.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('minute', 1, h.TIME)\nGROUP BY\n    h.TIME,\n    h.MACHINE,\n    h.latency_p99\n-- HAVING\n    -- SUM(r.op_get_range) < 100000\n--     AND SUM(r.op_nexts) < 10000\n--     AND SUM(s.ss_get_range) > 100\nORDER BY\n    h.TIME DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f82f4d01-80a3-4115-a726-7897e77ab6d1",
   "metadata": {
    "language": "python",
    "name": "latency_get_range_plot"
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Get data from latency_get_range SQL cell\ndf_get_range = latency_get_range.to_pandas()\n\n# Create boolean mask for high operation count (increased to 100K)\nhigh_op_mask = df_get_range['TOTAL_OP_GET_RANGE_1MIN_BEFORE'] > 100000\n\n# Create figure with dual y-axes\nfig, ax1 = plt.subplots(figsize=(15, 8))\n\n# Plot latency on primary y-axis with conditional coloring\nax1.set_xlabel('Time')\nax1.set_ylabel('Latency P99 (seconds)', color='red')\n\n# Plot points based on operation count threshold (smaller dots)\nmatch_data = df_get_range[high_op_mask]\nnot_match_data = df_get_range[~high_op_mask]\n\nif not match_data.empty:\n    ax1.scatter(match_data['HIGH_LATENCY_TIME'], match_data['LATENCY_P99'], \n               color='red', alpha=0.8, s=20, label='Match (>100K ops)', marker='o')\n\nif not not_match_data.empty:\n    ax1.scatter(not_match_data['HIGH_LATENCY_TIME'], not_match_data['LATENCY_P99'], \n               color='orange', alpha=0.8, s=20, label='Not Match (‚â§100K ops)', marker='x')\n\nax1.tick_params(axis='y', labelcolor='red')\nax1.tick_params(axis='x', rotation=45)\n\n# Create second y-axis for operation count\nax2 = ax1.twinx()\nax2.set_ylabel('Total Op Get Range (1min before)', color='blue')\n\n# Plot operation counts with same conditional coloring (smaller dots)\nif not match_data.empty:\n    ax2.scatter(match_data['HIGH_LATENCY_TIME'], match_data['TOTAL_OP_GET_RANGE_1MIN_BEFORE'], \n               color='darkblue', alpha=0.8, s=20, marker='o')\n\nif not not_match_data.empty:\n    ax2.scatter(not_match_data['HIGH_LATENCY_TIME'], not_match_data['TOTAL_OP_GET_RANGE_1MIN_BEFORE'], \n               color='lightblue', alpha=0.8, s=20, marker='x')\n\nax2.tick_params(axis='y', labelcolor='blue')\n\n# Add horizontal line at 100,000 threshold\nax2.axhline(y=100000, color='gray', linestyle='--', alpha=0.7, label='100K threshold')\n\n# Add title and grid\nplt.title('Latency Spikes vs Operation Load (Match: >100K ops, Not Match: ‚â§100K ops)')\nax1.grid(True, alpha=0.3)\n\n# Add legends\nax1.legend(loc='upper left')\nax2.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate summary with percentages\nmatch_count = len(match_data)\nnot_match_count = len(not_match_data)\ntotal_count = match_count + not_match_count\n\nmatch_percentage = (match_count / total_count * 100) if total_count > 0 else 0\nnot_match_percentage = (not_match_count / total_count * 100) if total_count > 0 else 0\n\n# Print summary with percentages\nprint(f\"Match (>100K ops): {match_count} events ({match_percentage:.1f}%)\")\nprint(f\"Not Match (‚â§100K ops): {not_match_count} events ({not_match_percentage:.1f}%)\")\nprint(f\"Total events: {total_count}\")\n\n# Also create Streamlit version\nst.subheader(\"Latency vs Operation Load Analysis\")\nst.write(f\"**Match (>100K ops):** {match_count} events ({match_percentage:.1f}%) | **Not Match (‚â§100K ops):** {not_match_count} events ({not_match_percentage:.1f}%)\")\nst.write(f\"**Total events:** {total_count}\")\nst.write(\"Red circles = Match, Orange X = Not Match\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16abe840-396a-4754-a98b-f95e6e926b76",
   "metadata": {
    "language": "python",
    "name": "latency_iterator_next"
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Get data from latency_get_range SQL cell and filter for TOTAL_OP_GET_RANGE_1MIN_BEFORE < 100000\ndf_nexts = latency_get_range.to_pandas()\ndf_filtered = df_nexts[df_nexts['TOTAL_OP_GET_RANGE_1MIN_BEFORE'] < 100000].copy()\n\n# Create boolean mask for high op_nexts count (match when > 100000)\nhigh_nexts_mask = df_filtered['TOTAL_OP_NEXTS_1MIN_BEFORE'] > 100000\n\n# Create figure with dual y-axes\nfig, ax1 = plt.subplots(figsize=(15, 8))\n\n# Plot latency on primary y-axis with conditional coloring\nax1.set_xlabel('Time')\nax1.set_ylabel('Latency P99 (seconds)', color='red')\n\n# Plot points based on op_nexts threshold\nmatch_data = df_filtered[high_nexts_mask]\nnot_match_data = df_filtered[~high_nexts_mask]\n\nif not match_data.empty:\n    ax1.scatter(match_data['HIGH_LATENCY_TIME'], match_data['LATENCY_P99'], \n               color='red', alpha=0.8, s=20, label='Match (>100K nexts)', marker='o')\n\nif not not_match_data.empty:\n    ax1.scatter(not_match_data['HIGH_LATENCY_TIME'], not_match_data['LATENCY_P99'], \n               color='orange', alpha=0.8, s=20, label='Not Match (‚â§100K nexts)', marker='x')\n\nax1.tick_params(axis='y', labelcolor='red')\nax1.tick_params(axis='x', rotation=45)\n\n# Create second y-axis for op_nexts count\nax2 = ax1.twinx()\nax2.set_ylabel('Total Op Nexts (1min before)', color='blue')\n\n# Plot op_nexts counts with same conditional coloring\nif not match_data.empty:\n    ax2.scatter(match_data['HIGH_LATENCY_TIME'], match_data['TOTAL_OP_NEXTS_1MIN_BEFORE'], \n               color='darkblue', alpha=0.8, s=20, marker='o')\n\nif not not_match_data.empty:\n    ax2.scatter(not_match_data['HIGH_LATENCY_TIME'], not_match_data['TOTAL_OP_NEXTS_1MIN_BEFORE'], \n               color='lightblue', alpha=0.8, s=20, marker='x')\n\nax2.tick_params(axis='y', labelcolor='blue')\n\n# Add horizontal line at 100,000 threshold\nax2.axhline(y=100000, color='gray', linestyle='--', alpha=0.7, label='100K nexts threshold')\n\n# Add title and grid\nplt.title('Latency Spikes vs Op Nexts (Filtered: Op Get Range < 100K)')\nax1.grid(True, alpha=0.3)\n\n# Add legends\nax1.legend(loc='upper left')\nax2.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate summary with percentages\nmatch_count = len(match_data)\nnot_match_count = len(not_match_data)\ntotal_count = match_count + not_match_count\n\nmatch_percentage = (match_count / total_count * 100) if total_count > 0 else 0\nnot_match_percentage = (not_match_count / total_count * 100) if total_count > 0 else 0\n\n# Print summary with percentages\nprint(f\"Filtered dataset (Op Get Range < 100K): {total_count} events\")\nprint(f\"Match (>100K nexts): {match_count} events ({match_percentage:.1f}%)\")\nprint(f\"Not Match (‚â§100K nexts): {not_match_count} events ({not_match_percentage:.1f}%)\")\n\n# Also create Streamlit version\nst.subheader(\"Iterator Next Operations Analysis (Filtered)\")\nst.write(f\"**Filtered dataset (Op Get Range < 100K):** {total_count} events\")\nst.write(f\"**Match (>100K nexts):** {match_count} events ({match_percentage:.1f}%) | **Not Match (‚â§100K nexts):** {not_match_count} events ({not_match_percentage:.1f}%)\")\nst.write(\"Red circles = Match, Orange X = Not Match\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "sql_test"
   },
   "source": "-- Welcome to Snowflake Notebooks!\n-- Try out a SQL cell to generate some data.\nselect\n  time_slice(time, 300, 'second') as TIMEBUCKET,\n  machine,\n  sum(\n      iff(\n          Type = 'RedwoodMetrics',\n          v:OpGetRange::float,\n          null\n      )\n  ) as query_num,\n  max(\n      iff(\n          Type = 'NormalReadLatencyMetrics',\n          v:\"P99\"::float,\n          null\n      )\n  ) as latency_p99,\n  sum(iff(Type = 'StorageMetrics', split_part(v:FetchRangeReadBytes, ' ', 1)::float, null)) as fetch_bytes,\n  sum(iff(Type = 'StorageMetrics', split_part(v:FetchRangeReadCount, ' ', 1)::float, null)) as fetch_count,\n  sum(iff(Type = 'StorageMetrics', split_part(v:RowsQueried, ' ', 1)::float, null)) as rows_read,\n  iff(\n    sum(\n      iff(\n        Type = 'RedwoodMetrics',\n        v:OpGetRange::float,\n        null\n      )\n    ) = 0,\n    null,\n    sum(iff(Type = 'StorageMetrics', split_part(v:FetchRangeReadBytes, ' ', 1)::float, null)) /\n    sum(\n      iff(\n        Type = 'RedwoodMetrics',\n        v:OpGetRange::float,\n        null\n      )\n    )\n  ) as bytes_per_query,\n    iff(\n    sum(\n      iff(\n        Type = 'RedwoodMetrics',\n        v:OpGetRange::float,\n        null\n      )\n    ) = 0,\n    null,\n    sum(iff(Type = 'StorageMetrics', split_part(v:RowsQueried, ' ', 1)::float, null)) /\n    sum(\n      iff(\n        Type = 'RedwoodMetrics',\n        v:OpGetRange::float,\n        null\n      )\n    )\n  ) as rows_per_query\nfrom\n  SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\nwhere\n  Type in ('RedwoodMetrics', 'NormalReadLatencyMetrics', 'StorageMetrics')\n  and log_group = 'prod1_autoprovision_88_100_fdbserver'\n  and TIMEBUCKET between '2025-11-01 00:00:00'::timestamp_ntz\n  and '2025-11-06 00:00:00'::timestamp_ntz\ngroup by\n  TIMEBUCKET, machine\nhaving\n  latency_p99 > 1\norder by latency_p99 desc;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ea2b86ac-1f00-4e92-9dda-71cf52b0042a",
   "metadata": {
    "language": "sql",
    "name": "sql_test2"
   },
   "outputs": [],
   "source": "WITH high_latency_events AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:\"P99\"::float as latency_p99\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'NormalReadLatencyMetrics'\n        AND v:\"P99\"::float > 0.1\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n),\nredwood_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:OpGetRange::float as op_get_range,\n        COALESCE(v:OpTryNext::float, 0) + \n        COALESCE(v:OpNext::float, 0) + \n        COALESCE(v:OpNextBatch::float, 0) as op_nexts\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'RedwoodMetrics'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n),\nstorage_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        split_part(v:ClearRangeMutations, ' ', 1)::float as clear_range_mutations,\n        split_part(v:GetRangeQueries, ' ', 1)::float as ss_get_range\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'StorageMetrics'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n        AND v:ClearRangeMutations IS NOT NULL\n)\nSELECT\n    h.TIME as high_latency_time,\n    h.MACHINE,\n    h.latency_p99,\n    SUM(r.op_get_range) as total_op_get_range_1min_before,\n    SUM(r.op_nexts) as total_op_nexts_1min_before,\n    SUM(s.clear_range_mutations) as total_clear_range_mutations_1min_before,\n    SUM(s.ss_get_range) as total_ss_get_range_1min_before\nFROM\n    high_latency_events h\n    LEFT JOIN redwood_metrics r ON h.MACHINE = r.MACHINE\n        AND r.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('second', 5, h.TIME)\n    LEFT JOIN storage_metrics s ON h.MACHINE = s.MACHINE\n        AND s.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('second', 5, h.TIME)\nGROUP BY\n    h.TIME,\n    h.MACHINE,\n    h.latency_p99\n-- HAVING\n--     SUM(r.op_get_range) < 10000\n--     AND SUM(r.op_nexts) < 10000\n--     AND SUM(s.ss_get_range) > 100\nORDER BY\n    h.TIME DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b77f0584-0f61-44f4-a64f-293b866377a5",
   "metadata": {
    "language": "sql",
    "name": "meta_store"
   },
   "outputs": [],
   "source": "WITH high_latency_events AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:\"P99\"::float as latency_p99\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_MAIN_V\n    WHERE\n        Type = 'NormalReadLatencyMetrics'\n        AND v:\"P99\"::float > 0.05\n        AND log_group = 'prod1fdb2_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n),\nredwood_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:OpGetRange::float as op_get_range,\n        COALESCE(v:OpTryNext::float, 0) + \n        COALESCE(v:OpNext::float, 0) + \n        COALESCE(v:OpNextBatch::float, 0) as op_nexts,\n        v:OpNewIterator::float as op_it_new,\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_MAIN_V\n    WHERE\n        Type = 'RedwoodMetrics'\n        AND log_group = 'prod1fdb2_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n),\nstorage_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        split_part(v:ClearRangeMutations, ' ', 1)::float as clear_range_mutations,\n        split_part(v:GetRangeQueries, ' ', 1)::float as ss_get_range\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_MAIN_V\n    WHERE\n        Type = 'StorageMetrics'\n        AND log_group = 'prod1fdb2_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n        AND v:ClearRangeMutations IS NOT NULL\n)\nSELECT\n    h.TIME as high_latency_time,\n    h.MACHINE,\n    h.latency_p99,\n    SUM(r.op_get_range) as total_op_get_range_1min_before,\n    SUM(r.op_nexts) as total_op_nexts_1min_before,\n    SUM(s.clear_range_mutations) as total_clear_range_mutations_1min_before,\n    SUM(s.ss_get_range) as total_ss_get_range_1min_before,\n    SUM(r.op_it_new) as total_op_it_news_1min_before\nFROM\n    high_latency_events h\n    LEFT JOIN redwood_metrics r ON h.MACHINE = r.MACHINE\n        AND r.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('second', 5, h.TIME)\n    LEFT JOIN storage_metrics s ON h.MACHINE = s.MACHINE\n        AND s.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('second', 5, h.TIME)\nGROUP BY\n    h.TIME,\n    h.MACHINE,\n    h.latency_p99\nHAVING\n    SUM(r.op_get_range) > 100000\n    -- AND SUM(r.op_nexts) < 10000\n    -- AND SUM(s.ss_get_range) > 100\nORDER BY\n    h.TIME DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2b81f8f-f1f1-446f-9f70-fdfdbd9e9572",
   "metadata": {
    "language": "sql",
    "name": "slow_tasks"
   },
   "outputs": [],
   "source": "WITH slow_tasks AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:Duration::float as duration,\n        v:NumYields as yield,\n        v\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'SlowTask'\n        AND v:Roles = 'SS'\n        AND v:TaskID = '2500'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n),\nlatency_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:\"P99\"::float as latency_p99\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'NormalReadLatencyMetrics'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n)\nSELECT\n    st.TIME,\n    st.MACHINE,\n    st.duration,\n    st.yield,\n    st.v,\n    MAX(lm.latency_p99) as max_latency_p99_1min_before\nFROM\n    slow_tasks st\n    LEFT JOIN latency_metrics lm ON st.MACHINE = lm.MACHINE\n        AND lm.TIME BETWEEN DATEADD('minute', -1, st.TIME)\n        AND DATEADD('second', 5, st.TIME)\nGROUP BY\n    st.TIME,\n    st.MACHINE,\n    st.duration,\n    st.yield,\n    st.v\nORDER BY st.duration DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28034457-6296-4ded-92bf-0c94784389ab",
   "metadata": {
    "language": "sql",
    "name": "sql_data_with_slow_tasks"
   },
   "outputs": [],
   "source": "WITH high_latency_events AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:\"P99\"::float as latency_p99\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'NormalReadLatencyMetrics'\n        AND v:\"P99\"::float > 0.5\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n),\nredwood_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:OpGetRange::float as op_get_range,\n        COALESCE(v:OpTryNext::float, 0) + \n        COALESCE(v:OpNext::float, 0) + \n        COALESCE(v:OpNextBatch::float, 0) as op_nexts\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'RedwoodMetrics'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n),\nstorage_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        split_part(v:ClearRangeMutations, ' ', 1)::float as clear_range_mutations,\n        split_part(v:GetRangeQueries, ' ', 1)::float as ss_get_range\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'StorageMetrics'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n        AND v:ClearRangeMutations IS NOT NULL\n),\nslow_task_metrics AS (\n    SELECT\n        TIME,\n        MACHINE,\n        v:Duration::float as duration\n    FROM\n        SNOWHOUSE_IMPORT.PROD1.FDB_SERVER_LOGS_OLTP_V\n    WHERE\n        Type = 'SlowTask'\n        AND log_group = 'prod1_autoprovision_88_100_fdbserver'\n        AND TIME BETWEEN '2025-10-31 00:00:00'::timestamp_ntz\n        AND '2025-11-06 00:00:00'::timestamp_ntz\n        AND v:Duration IS NOT NULL\n)\nSELECT\n    h.TIME as high_latency_time,\n    h.MACHINE,\n    h.latency_p99,\n    SUM(r.op_get_range) as total_op_get_range_1min_before,\n    SUM(r.op_nexts) as total_op_nexts_1min_before,\n    SUM(s.clear_range_mutations) as total_clear_range_mutations_1min_before,\n    SUM(s.ss_get_range) as total_ss_get_range_1min_before,\n    SUM(st.duration) as total_slow_task_duration_1min_before\nFROM\n    high_latency_events h\n    LEFT JOIN redwood_metrics r ON h.MACHINE = r.MACHINE\n        AND r.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('second', 5, h.TIME)\n    LEFT JOIN storage_metrics s ON h.MACHINE = s.MACHINE\n        AND s.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('second', 5, h.TIME)\n    LEFT JOIN slow_task_metrics st ON h.MACHINE = st.MACHINE\n        AND st.TIME BETWEEN DATEADD('minute', -1, h.TIME)\n        AND DATEADD('second', 5, h.TIME)\nGROUP BY\n    h.TIME,\n    h.MACHINE,\n    h.latency_p99\n-- HAVING\n    -- SUM(r.op_get_range) < 100000\n--     AND SUM(r.op_nexts) < 10000\n--     AND SUM(s.ss_get_range) > 100\nORDER BY\n    h.TIME DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f49b638-d9bb-44c6-ba98-d7110b741f2d",
   "metadata": {
    "language": "python",
    "name": "slow_tasks_py"
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport json\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Get data from slow_tasks SQL cell\ndf_slow_tasks = slow_tasks.to_pandas()\n\n# Parse the JSON data in the V column to extract TaskID and Duration\ntask_data = []\nfor index, row in df_slow_tasks.iterrows():\n    try:\n        # Parse the JSON string in the V column\n        v_data = json.loads(row['V'])\n        task_id = v_data.get('TaskID', 'Unknown')\n        duration = v_data.get('Duration', 0)\n        \n        task_data.append({\n            'TaskID': task_id,\n            'Duration': float(duration) if duration else 0\n        })\n    except (json.JSONDecodeError, TypeError, ValueError):\n        # Skip rows with invalid JSON or missing data\n        continue\n\n# Convert to DataFrame\ndf_tasks = pd.DataFrame(task_data)\n\n# Group by TaskID and sum Duration\ntask_summary = df_tasks.groupby('TaskID')['Duration'].sum().reset_index()\n\n# Sort by Duration in descending order\ntask_summary = task_summary.sort_values('Duration', ascending=False)\n\n# Display the results\nprint(\"Slow Tasks Summary - Grouped by TaskID, Ordered by Total Duration:\")\nprint(task_summary.to_string(index=False))\n\n# Also create Streamlit visualization\nst.subheader(\"Slow Tasks Analysis\")\nst.write(\"**Tasks grouped by TaskID, ordered by total duration:**\")\nst.dataframe(task_summary)\n\n# Create a bar chart\nif not task_summary.empty:\n    st.bar_chart(task_summary.set_index('TaskID')['Duration'])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "# Then, we can use the python name to turn cell2 into a Pandas dataframe\nmy_df = cell2.to_pandas()\n\n# Chart the data\nst.subheader(\"Chance of SNOW ‚ùÑÔ∏è\")\nst.line_chart(my_df, x='SNOWDAY', y='CHANCE_OF_SNOW')\n\n# Give it a go!\nst.subheader(\"Try it out yourself and show off your skills ü•á\")",
   "execution_count": null,
   "outputs": []
  }
 ]
}